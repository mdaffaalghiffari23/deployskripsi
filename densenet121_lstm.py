# -*- coding: utf-8 -*-
"""DenseNet121_LSTM

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ng63Q3rnuOih50HBn0h_B-9FEDGcbO2T

# Library
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os
import time
import re

import torch
import torch.nn as nn
import torchvision.models as models
import torchvision.transforms as transforms
import numpy as np
import itertools
import nltk
from PIL import Image
from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler
from nltk.tokenize import word_tokenize
from nltk.translate.bleu_score import sentence_bleu, corpus_bleu
from tqdm import tqdm
from sklearn.model_selection import train_test_split, KFold

nltk.download('punkt_tab')

"""# Load Dataset"""

#import gdrive
from google.colab import drive
drive.mount('/content/drive')

img_dir = '/content/drive/MyDrive/Dataset/new_train_images'
data_path = '/content/drive/MyDrive/Dataset/train.csv'

df = pd.read_csv(data_path, usecols=['id_code', 'caption'])

df.info()

df.sort_values(by='id_code', inplace=True)
df.reset_index(drop=True, inplace=True)
df

df['caption'].value_counts()

img_list = os.listdir(img_dir)
df_img = pd.DataFrame(img_list, columns=['img_name'])
df_img.sort_values(by='img_name', inplace=True)
df_img.reset_index(drop=True, inplace=True)
df_img

df['img_name'] = df_img['img_name']
df

"""# Setup

## Load Data
"""

def load_data(img_id, img_caption):
    # Load image IDs
    img_id = img_id.values.tolist()

    # Load captions
    img_caption = img_caption.values.tolist()

    print(f"Loaded {len(img_id)} image IDs")
    print(f"Sample image IDs: {img_id[:5]}")
    print(f"Loaded {len(img_caption)} captions")
    print(f"Sample captions: {img_caption[:5]}")

    # Create a combined dataset
    combined_data = []
    for i in range(len(img_id)):
        if i < len(img_caption):
            combined_data.append(f"{img_id[i]}\t{img_caption[i]}")

    print(f"Created {len(combined_data)} combined entries")
    print(f"Sample combined data: {combined_data[:2]}")

    return img_id, combined_data

"""## Pre-processing for captions

"""

def process_captions(img_caption):
    # Create dictionary mapping image IDs to captions
    imgcaption_dict = {}
    for i in range(0, len(img_caption), 1):
        cap = img_caption[i].strip()
        cap = cap.split('\t')
        imgcaption_dict[cap[0]] = re.sub(r'[^a-zA-Z0-9\s]', '', cap[1]).strip().lower().replace('  ', ' ') #hilangin special karakter

    # Tokenize captions
    imgcaption_token = []
    imgcaption_wordtoken = []
    for (i, j) in imgcaption_dict.items():
        imgcaption_token.append([i, nltk.word_tokenize(j)])
        imgcaption_wordtoken.append(nltk.word_tokenize(j))

    # Create vocabulary
    alltokens = itertools.chain.from_iterable(imgcaption_wordtoken)
    wordtoid = {token: idx for idx, token in enumerate(set(alltokens))}

    alltokens = itertools.chain.from_iterable(imgcaption_wordtoken)
    idtoword = [token for idx, token in enumerate(set(alltokens))]
    idtoword = np.asarray(idtoword)

    # Convert tokens to IDs
    imgcaption_token_id = [[wordtoid.get(token, -4) + 4 for token in x[1]] for x in imgcaption_token]

    # Add special tokens
    wordtoid['<unknown>'] = 0
    wordtoid['<start>'] = 1
    wordtoid['<end>'] = 2
    wordtoid['<pad>'] = 3

    # Adjust token IDs
    for (_, i) in wordtoid.items():
        if i < 0:  # Only adjust the special tokens we just added
            wordtoid[_] = i + 4

    # Create ID to word mapping
    idtoword_dict = {}
    cnt = 4
    for i in idtoword:
        idtoword_dict[cnt] = i
        cnt += 1
    idtoword_dict[0] = '<unknown>'
    idtoword_dict[1] = '<start>'
    idtoword_dict[2] = '<end>'
    idtoword_dict[3] = '<pad>'

    # Calculate caption lengths
    imgcaption_length = {}
    for i in imgcaption_token:
        imgcaption_length[i[0]] = len(i[1]) + 2  # +2 for <start> and <end>

    # Add <start> and <end> tokens
    for i in imgcaption_token_id:
        i.insert(0, wordtoid['<start>'])
        i.append(wordtoid['<end>'])

    # Find max caption length
    length = []
    for (_, j) in imgcaption_length.items():
        length.append(j)
    max_len = max(length)
    print(f"Maximum caption length pre-process: {max_len}")

    # Pad captions to max length
    for n, i in enumerate(imgcaption_token):
        if imgcaption_length[i[0]] < max_len:
            imgcaption_token_id[n].extend([wordtoid['<pad>']] * (max_len - imgcaption_length[i[0]]))

    # Create dictionary mapping image IDs to token IDs
    imgcaption_token_id_dict = {}
    for n, i in enumerate(imgcaption_token):
        imgcaption_token_id_dict[i[0]] = imgcaption_token_id[n]

    # Create dictionary mapping image IDs to original tokens (for BLEU score)
    imgcaption_original_tokens = {}
    for i in imgcaption_token:
        imgcaption_original_tokens[i[0]] = i[1]

    # Get list of valid image IDs (those with captions)
    valid_img_ids = list(imgcaption_token_id_dict.keys())

    return imgcaption_token_id_dict, imgcaption_length, wordtoid, idtoword_dict, max_len, imgcaption_original_tokens, valid_img_ids

"""## Create Dataset"""

class FundusDataset(Dataset):
    def __init__(self, img_dir, img_id, caption, caption_length):
        self.img_dir = img_dir
        self.img_id = img_id
        self.caption = caption
        self.caption_length = caption_length

    def __len__(self):
        return len(self.img_id)

    def __getitem__(self, index):
        image_id = self.img_id[index]
        image = Image.open(os.path.join(self.img_dir,image_id))

        # Get caption and length
        caption = np.array(self.caption[image_id])
        caption_length = self.caption_length[image_id]

        return image, image_id, caption, caption_length

"""## Tranforms Dataset"""

class TransformDataset(Dataset):
    def __init__(self, dataset, transform=None):
        self.dataset = dataset
        self.transform = transform

    def __len__(self):
        return len(self.dataset)

    def __getitem__(self, index):
        image, image_id, caption, caption_length = self.dataset[index]

        # Apply transform if provided
        if self.transform is not None:
            image = self.transform(image)

        return image, image_id, caption, caption_length

"""## DenseNet+LSTM Architecture"""

class DenseNetLSTM(nn.Module):
    def __init__(self, embed_size, hidden_size, vocab_size):
        super(DenseNetLSTM, self).__init__()

        # Load pretrained DenseNet121
        densenet = models.densenet121(weights='IMAGENET1K_V1')
        # Remove the final fully connected layer
        self.densenet = nn.Sequential(*list(densenet.children())[:-1])

        # Freeze DenseNet parameters (optional, set to False for fine-tuning)
        for param in self.densenet.parameters():
            param.requires_grad = False

        # Add an adaptive pooling layer to ensure consistent feature size
        self.adaptive_pool = nn.AdaptiveAvgPool2d((1, 1))

        # DenseNet121 output features
        densenet_output_features = 1024

        # Feature transformation layers
        self.fc1 = nn.Linear(densenet_output_features, embed_size)
        self.bn1d = nn.BatchNorm1d(embed_size)

        # Word embedding
        self.wordembed = nn.Embedding(vocab_size, embed_size)

        # LSTM for caption generation
        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)

        # Output layer
        self.fc2 = nn.Linear(hidden_size, vocab_size)

    def forward(self, images, captions):
        """Training forward pass"""
        # Extract features from image using DenseNet
        features = self.densenet(images)

        # Apply adaptive pooling and flatten
        features = self.adaptive_pool(features).squeeze(3).squeeze(2)

        # Print feature shape for debugging
        # print(f"Feature shape after pooling: {features.shape}")

        # Apply feature transformation
        features = self.bn1d(self.fc1(features))

        # Prepare captions (remove <end> token for input)
        captions = captions[:, :-1]

        # Embed word indices
        embed = self.wordembed(captions)

        # Concatenate image features with word embeddings
        inputs = torch.cat((features.unsqueeze(1), embed), 1)

        # Pass through LSTM
        lstmout, hidden = self.lstm(inputs)

        # Get outputs
        outputs = self.fc2(lstmout)

        return outputs

    def sample(self, inputs, states=None, max_len=50):
        """Inference forward pass"""
        # Extract features from image using DenseNet
        feat = self.densenet(inputs)

        # Apply adaptive pooling and flatten
        feat = self.adaptive_pool(feat).squeeze(3).squeeze(2)

        # Apply feature transformation
        feat = self.bn1d(self.fc1(feat))

        # Initialize input with image features
        features = feat.unsqueeze(1)

        predicted_sentence = []

        # Generate caption word by word
        for i in range(max_len):
            lstm_out, states = self.lstm(features, states)
            lstm_out = lstm_out.squeeze(1)
            outputs = self.fc2(lstm_out)
            target = outputs.max(1)[1]
            predicted_sentence.append(target.item())

            # Stop if <end> token is predicted
            if target.item() == 2:  # <end> token
                break

            # Update input for next step
            features = self.wordembed(target).unsqueeze(1)

        return predicted_sentence

"""## Calculate Accuracy"""

def calculate_accuracy(outputs, targets, pad_idx=3):
    """
    Calculate word-level and sentence-level accuracy.

    Args:
        outputs: Model outputs [batch_size, seq_len, vocab_size]
        targets: Target captions [batch_size, seq_len]
        pad_idx: Index of padding token

    Returns:
        word_accuracy: Accuracy of word predictions
    """
    # Get predictions
    _, preds = outputs.max(dim=2)

    # Create mask to ignore padding
    mask = (targets != pad_idx)

    # Calculate word-level accuracy
    correct_words = ((preds == targets) & mask).sum().item()
    total_words = mask.sum().item()
    word_accuracy = correct_words / total_words if total_words > 0 else 0

    return word_accuracy

"""## Training function"""

def train_model(model, train_loader, criterion, optimizer, device, epoch, vocab_size):
    model.train()
    total_loss = 0
    word_acc_total = 0
    batches = 0

    for i, (images, _, captions, _) in enumerate(train_loader):
        # Move data to device
        images = images.to(device)
        captions = captions.to(device)

        # Zero the gradients
        optimizer.zero_grad()

        # Forward pass
        outputs = model(images, captions)

        # Prepare targets (shift right to predict next word)
        targets = captions[:, 1:]  # Remove <start> token

        # Make sure outputs has the right shape
        outputs = outputs[:, :targets.size(1), :]

        # Calculate accuracy
        word_acc = calculate_accuracy(outputs, targets)
        word_acc_total += word_acc
        batches += 1

        # Reshape for loss calculation
        outputs_flat = outputs.reshape(-1, vocab_size)
        targets_flat = targets.reshape(-1)

        # Calculate loss
        loss = criterion(outputs_flat, targets_flat)

        # Backward pass
        loss.backward()

        # Clip gradients to avoid exploding gradients
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

        # Update weights
        optimizer.step()

        # Update statistics
        total_loss += loss.item()

        if (i + 1) % 100 == 0:
            print(f'Epoch [{epoch+1}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}, '
                  f'Word Acc: {word_acc:.4f}, Sentence Acc: {sentence_acc:.4f}')

    # Calculate average loss and accuracy
    avg_loss = total_loss / len(train_loader)
    avg_word_acc = word_acc_total / batches

    return avg_loss, avg_word_acc

"""## Validation function"""

def validate_model(model, val_loader, criterion, device, vocab_size, idtoword_dict, original_tokens=None):
    model.eval()
    total_loss = 0
    word_acc_total = 0
    batches = 0

    all_bleu1 = 0
    total_samples = 0

    with torch.no_grad():
        for i, (images, image_ids, captions, _) in enumerate(val_loader):
            # Move data to device
            images = images.to(device)
            captions = captions.to(device)

            # Forward pass
            outputs = model(images, captions)

            # Prepare targets (shift right to predict next word)
            targets = captions[:, 1:]  # Remove <start> token

            # Make sure outputs has the right shape
            outputs = outputs[:, :targets.size(1), :]

            # Calculate accuracy
            word_acc = calculate_accuracy(outputs, targets)
            word_acc_total += word_acc
            batches += 1

            # Reshape for loss calculation
            outputs_flat = outputs.reshape(-1, vocab_size)
            targets_flat = targets.reshape(-1)

            # Calculate loss
            loss = criterion(outputs_flat, targets_flat)

            # Update statistics
            total_loss += loss.item()

            # Calculate BLEU scores if reference tokens are provided
            if original_tokens is not None:
                batch_size = images.size(0)
                for j in range(batch_size):
                    # Generate caption
                    image = images[j:j+1]
                    image_id = image_ids[j]

                    # Get reference tokens
                    if image_id in original_tokens:
                        reference = original_tokens[image_id]

                        # Generate caption
                        prediction = model.sample(image, max_len=11)

                        # Convert prediction to tokens
                        candidate = []
                        for idx in prediction:
                            if idx == 2:  # <end> token
                                break
                            if idx not in [0, 1, 3]:  # Skip <unknown>, <start>, and <pad>
                                candidate.append(idtoword_dict[idx])

                        # Calculate BLEU scores
                        bleu1 = sentence_bleu([reference], candidate, weights=(1, 0, 0, 0))

                        all_bleu1 += bleu1
                        total_samples += 1

    # Calculate average loss and accuracy
    avg_loss = total_loss / len(val_loader)
    avg_word_acc = word_acc_total / batches

    # Calculate average BLEU scores
    avg_bleu1 = all_bleu1 / total_samples if total_samples > 0 else 0

    return avg_loss, avg_word_acc, avg_bleu1

"""## Function to convert prediction to caption"""

def convert_prediction_to_caption(prediction, idtoword_dict):
    caption = []
    for idx in prediction:
        if idx == 2:  # <end> token
            break
        if idx not in [0, 1, 3]:  # Skip <unknown>, <start>, and <pad>
            caption.append(idtoword_dict[idx])
    return ' '.join(caption)

"""## K-fold cross-validation"""

def k_fold_cross_validation(dataset, k_folds, batch_size, embed_size, hidden_size, vocab_size,
                           learning_rate, num_epochs, device, idtoword_dict, max_len,
                           train_transform, val_transform, original_tokens=None):
    # Define KFold
    kfold = KFold(n_splits=k_folds, shuffle=True, random_state=42)

    # Store results from each fold
    fold_results = []

    # Track best model overall
    best_val_loss = float('inf')
    best_model_state = None
    best_fold = 0

    # For each fold
    for fold, (train_ids, val_ids) in enumerate(kfold.split(dataset)):
        print(f"FOLD {fold+1}/{k_folds}")
        print("-" * 50)
        print(f"Using 80% of training data: {len(train_ids)} samples")
        print(f"Using 20% of validation data: {len(val_ids)} samples")


        # Create samplers for train and validation sets
        train_sampler = SubsetRandomSampler(train_ids)
        val_sampler = SubsetRandomSampler(val_ids)

        # Create datasets with appropriate transforms
        train_dataset = TransformDataset(dataset, transform=train_transform)
        val_dataset = TransformDataset(dataset, transform=val_transform)

        # Create data loaders
        train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler)
        val_loader = DataLoader(val_dataset, batch_size=batch_size, sampler=val_sampler)

        # Initialize model
        model = DenseNetLSTM(embed_size, hidden_size, vocab_size)
        model = model.to(device)

        # Define loss function (ignore <pad> tokens)
        criterion = nn.CrossEntropyLoss(ignore_index=3)

        # Define optimizer
        optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate)

        # Define scheduler
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.5)

        # Early stopping parameters
        patience = 10
        early_stop_counter = 0
        fold_best_val_loss = float('inf')
        fold_best_model_state = None

        # Track fold metrics
        fold_train_losses = []
        fold_val_losses = []
        fold_val_word_accs = []
        fold_val_bleu1 = []

        # Training loop
        for epoch in range(num_epochs):
            # Train
            train_loss, train_word_acc = train_model(
                model, train_loader, criterion, optimizer, device, epoch, vocab_size
            )

            # Validate
            val_loss, val_word_acc, val_bleu1 = validate_model(
                model, val_loader, criterion, device, vocab_size, idtoword_dict, original_tokens
            )

            # Store metrics
            fold_train_losses.append(train_loss)
            fold_val_losses.append(val_loss)
            fold_val_word_accs.append(val_word_acc)
            fold_val_bleu1.append(val_bleu1)

            # Print results
            print(f'Epoch {epoch+1}/{num_epochs}:')
            print(f'Train Loss: {train_loss:.4f}, Word Acc: {train_word_acc:.4f}')
            print(f'Val Loss: {val_loss:.4f}, Word Acc: {val_word_acc:.4f}')
            print(f'Val BLEU-1: {val_bleu1:.4f}')

            # Adjust learning rate
            scheduler.step(val_loss)

            # Check if this is the best model for this fold
            if val_loss < fold_best_val_loss:
                fold_best_val_loss = val_loss
                fold_best_model_state = model.state_dict()

                # Save fold model
                os.makedirs('/kaggle/working/models', exist_ok=True)
                torch.save({
                    'epoch': epoch,
                    'model_state_dict': model.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                    'val_loss': val_loss,
                    'val_word_acc': val_word_acc,
                    'val_bleu1': val_bleu1
                }, f'/kaggle/working/models/fold_{fold+1}_best_model.pth')

                print('Model saved!')
                early_stop_counter = 0

                # Check if this is the best model overall
                if val_loss < best_val_loss:
                    best_val_loss = val_loss
                    best_model_state = model.state_dict()
                    best_fold = fold + 1
            else:
                early_stop_counter += 1
                print(f'Early stopping counter: {early_stop_counter}/{patience}')

            # Early stopping
            if early_stop_counter >= patience:
                print(f'Early stopping triggered after {epoch+1} epochs')
                break

            print("-" * 50)

        # Collect results for this fold
        fold_results.append({
            'fold': fold + 1,
            'best_val_loss': fold_best_val_loss,
            'best_val_word_acc': max(fold_val_word_accs),
            'best_val_bleu1': max(fold_val_bleu1),
            'train_losses': fold_train_losses,
            'val_losses': fold_val_losses,
            'val_word_accs': fold_val_word_accs,
            'val_bleu1': fold_val_bleu1
        })

        print(f"Fold {fold+1} complete!")
        print("=" * 80)

    # Save best overall model
    if best_model_state is not None:
        torch.save({
            'model_state_dict': best_model_state,
            'val_loss': best_val_loss,
            'fold': best_fold
        }, '/kaggle/working/models/best_overall_model.pth')
        print(f"Best overall model saved (from fold {best_fold})")

    # Print summary of results
    print("\nK-FOLD CROSS VALIDATION RESULTS")
    print("=" * 80)

    avg_val_loss = sum(fold['best_val_loss'] for fold in fold_results) / len(fold_results)
    avg_val_word_acc = sum(fold['best_val_word_acc'] for fold in fold_results) / len(fold_results)
    avg_val_bleu1 = sum(fold['best_val_bleu1'] for fold in fold_results) / len(fold_results)

    print(f"Average validation loss: {avg_val_loss:.4f}")
    print(f"Average validation word accuracy: {avg_val_word_acc:.4f}")
    print(f"Average validation BLEU-1: {avg_val_bleu1:.4f}")

    for i, fold in enumerate(fold_results):
        print(f"\nFold {i+1}:")
        print(f"  Best validation loss: {fold['best_val_loss']:.4f}")
        print(f"  Best validation word accuracy: {fold['best_val_word_acc']:.4f}")
        print(f"  Best validation BLEU-1: {fold['best_val_bleu1']:.4f}")

    return fold_results, best_model_state, best_fold

"""# Main Program"""

def main():
   # Load data
    img_id, img_caption = load_data(df['img_name'], df['caption'])

    # Process captions
    caption_dict, caption_length, wordtoid, idtoword_dict, max_len, original_tokens, img_id = process_captions(img_caption)

    # Set device
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    # Define separate transformations for training and validation
    train_transform = transforms.Compose([
        transforms.Resize((256, 256)),
        transforms.CenterCrop(224),
        transforms.RandomPosterize(bits=2),
        transforms.RandomAdjustSharpness(sharpness_factor=2),
        transforms.RandomAutocontrast(),
        transforms.RandomEqualize(),
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation(10),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])

    val_transform = transforms.Compose([
        transforms.Resize((256, 256)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])

    # Create dataset without transforms (transforms will be applied after splitting)
    dataset = FundusDataset(img_dir, img_id, caption_dict, caption_length)

    # Set hyperparameters
    embed_size = 256
    hidden_size = 512
    vocab_size = len(wordtoid)
    learning_rate = 0.001
    num_epochs = 100
    batch_size = 128
    k_folds = 5

    # Start k-fold cross validation
    fold_results, best_model_state, best_fold = k_fold_cross_validation(
        dataset, k_folds, batch_size, embed_size, hidden_size, vocab_size,
        learning_rate, num_epochs, device, idtoword_dict, max_len,
        train_transform, val_transform, original_tokens
    )

    # Load best model for testing
    print("\nTESTING BEST MODEL ON SAMPLE IMAGES")
    print("=" * 80)

    # Create test dataset with validation transform
    test_dataset = TransformDataset(dataset, transform=val_transform)

    # Create test loader with a few samples
    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=True, num_workers=0)
    test_iter = iter(test_loader)

    # Initialize model with best weights
    model = DenseNetLSTM(embed_size, hidden_size, vocab_size)
    model.load_state_dict(best_model_state)
    model = model.to(device)
    model.eval()

    # Test on a few samples
    with torch.no_grad():
        for _ in range(8):
            try:
                # Get a sample
                image, image_id, caption, _ = next(test_iter)
                image = image.to(device)

                # Generate caption
                prediction = model.sample(image, max_len=max_len)

                # Convert prediction to caption
                generated_caption = convert_prediction_to_caption(prediction, idtoword_dict)

                # Get ground truth caption
                ground_truth = " ".join(original_tokens[image_id[0]])

                # Print results
                print(f'Image ID: {image_id[0]}')
                print(f'Generated Caption: {generated_caption}')
                print(f'Ground Truth: {ground_truth}')
                print('-' * 50)
            except StopIteration:
                break

if __name__ == '__main__':
    main()

